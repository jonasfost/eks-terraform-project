name: EKS Terraform Provisioning

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  actions: read
  security-events: write

jobs:
  EKS-cluster-terraform-provisioning:
    # if: ${{ always() }}
    name: 'EKS Cluster Terraform Provisioning'
    runs-on: ubuntu-latest
    environment: production

    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
    defaults:
      run:
        shell: bash
        working-directory: "./terraform"

    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout
      uses: actions/checkout@v3

    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

    # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.
    - name: Terraform Init
      id: init
      run: terraform init

    # Validate the terraform files
    - name: Terraform Validate
      id: validate
      run: terraform validate -no-color

    # Checks that all Terraform configuration files adhere to a canonical format
    - name: Terraform Format
      id: fmt
      run: terraform fmt -check

    # Generates an execution plan for Terraform
    - name: Terraform Plan
      id: plan
      run: terraform plan -input=false -no-color

   # Add a comment to pull requests with plan results
    - name: add-plan-comment
      id: comment
      uses: actions/github-script@v3
      if: github.event_name == 'pull_request'
      env:
        PLAN: "terraform\n${{ steps.plan.outputs.stdout }}"
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const output = `#### Terraform Format and Style üñå\`${{ steps.fmt.outcome }}\`
          #### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
          #### Terraform Validation ü§ñ${{ steps.validate.outputs.stdout }}
          #### Terraform Plan üìñ\`${{ steps.plan.outcome }}\`
      
          <details><summary>Show Plan</summary>
          
          \`\`\`${process.env.PLAN}\`\`\`
          
          </details>
          
          *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`, Working Directory: \`${{ env.tf_actions_working_dir }}\`, Workflow: \`${{ github.workflow }}\`*`;
            
          github.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: output
          })

    # Scan Terraform Config for Vulnerabilities

    # - name: Run Trivy vulnerability scanner in IaC mode
    #   uses: aquasecurity/trivy-action@master
    #   with:
    #     scan-type: 'config'
    #     hide-progress: false
    #     format: 'sarif'
    #     output: 'trivy-results.sarif'
    #     # exit-code: '1'
    #     ignore-unfixed: true
    #     severity: 'MEDIUM,HIGH,CRITICAL'
        
    # - name: Upload Trivy scan results to GitHub Security tab
    #   uses: github/codeql-action/upload-sarif@v2
    #   with:
    #     sarif_file: 'trivy-results.sarif'

    # Snyk scan Infrastructure as Code Terraform Config for Vulnerabilities
    - name: Run Snyk scan to check configuration files for security issues
      # Snyk can be used to break the build when it detects security issues.
      # In this case we want to upload the issues to GitHub Code Scanning
      continue-on-error: true
      uses: snyk/actions/iac@master
      env:
        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
      # Sharing the test results to the Snyk platform
      with:
        args: --sarif-file-output=snyk.scan
    - name: Upload Snyk Scan Result to GitHub Code Scanning
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: snyk.scan

    # On push to "main", build or change infrastructure according to Terraform configuration files
    # Note: It is recommended to set up a required "strict" status check in your repository for "Terraform Cloud". See the documentation on "strict" 
    # required status checks for more information: https://help.github.com/en/github/administering-a-repository/types-of-required-status-checks
    
    - name: Terraform Apply
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: terraform apply -auto-approve -input=false

    - name: Terraform Destroy
      env:
        destroy: false
      if: ${{ env.destroy == 'true' }} 
      run: terraform destroy -auto-approve -input=false  

    # Install kubectl on the docker
    - name: Install kubectl on the container
      uses: azure/setup-kubectl@v3
      with:
        version: 'latest' # 'v1.24.0' # default is latest stable
      id: install

    # Configure AWS Credentials for docker access
		- name: Configure AWS Credentials
		  uses: aws-actions/configure-aws-credentials@v4
		  with:
        aws-access-key-id: ${{ env.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ env.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_DEFAULT_REGION }}

    # - name: Configure AWS Credentials
    #   uses: aws-actions/configure-aws-credentials@v4
    #   with:
    #     managed-session-policies: arn:aws:iam::aws:policy/AdministratorAccess
    #     aws-region: us-east-2

    # Kubernetes config update to allow kubectl to work within the context
		- name: Update kube config to allow kubectl to work within the context
      # run: aws eks update-kubeconfig --region ${{ env.AWS_DEFAULT_REGION }}  --name ${{ env.EKS_CLUSTER }}
      # run: aws eks update-kubeconfig --region $(terraform output -raw region) --name $(terraform output -raw cluster_name)
      run: |
        aws eks update-kubeconfig --region us-east-2 --name knote-eks

    # Verify the ckuster info
    - name: Get the information about the cluster
      shell: bash
      run: |
        kubectl cluster-info
    
    # Verify all the worker nodes are up
    - name: Verify all the cluster nodes
      shell: bash
      run: |
        kubectl get nodes

    # Login to DockerHub and pull the knote image
		- name: Login to Docker Hub and pull the knote image
		  uses: docker/login-action@v2
		  with:
        username: ${{ env.DOCKER_USER }}
        password: ${{ env.DOCKER_PASSWORD }}
      run: |
        docker pull ${{env.DOCKER_USER}}/${{env.DOCKER_REPOSITORY}}:${{env.NODE_IMAGE_TAG}}
    
    # Deploy the app to EKS
		- name: Deploy knote application to EKS
		  shell: bash
      run: |
			  kubectl apply -f kube
        kubectl apply -f ingress.yml
			
		# Verify the deployment
		- name: Verify deployment
		  shell: bash
      run: |
        kubectl get pods
